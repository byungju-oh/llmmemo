# ëŒ€í™” ê¸°ì–µ ì—ì´ì „íŠ¸ êµ¬í˜„
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
# pip install chromadb sentence-transformers google-generativeai streamlit

import os
import json
import datetime
from typing import List, Dict, Any
import chromadb
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import streamlit as st

class MemoryAgent:
    def __init__(self, gemini_api_key: str):
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¡œì»¬ ì €ì¥)
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        
        # ì»¬ë ‰ì…˜ ìƒì„±/ë¡œë“œ
        self.collection = self.chroma_client.get_or_create_collection(
            name="conversation_memory",
            metadata={"hnsw:space": "cosine"}
        )
        
        # ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Gemini API ì„¤ì •
        genai.configure(api_key=gemini_api_key)
        self.llm = genai.GenerativeModel('gemini-pro')
        
        # ëŒ€í™” ID ì¹´ìš´í„°
        self.conversation_id = 0
    
    def create_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        return self.embedding_model.encode(text).tolist()
    
    def store_conversation(self, user_input: str, assistant_response: str):
        """ëŒ€í™”ë¥¼ ë²¡í„° DBì— ì €ì¥"""
        timestamp = datetime.datetime.now().isoformat()
        
        # ì‚¬ìš©ì ì…ë ¥ ì €ì¥
        user_embedding = self.create_embedding(user_input)
        user_id = f"user_{self.conversation_id}"
        
        self.collection.add(
            embeddings=[user_embedding],
            documents=[user_input],
            metadatas=[{
                "type": "user_input",
                "timestamp": timestamp,
                "conversation_id": self.conversation_id
            }],
            ids=[user_id]
        )
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì €ì¥
        assistant_embedding = self.create_embedding(assistant_response)
        assistant_id = f"assistant_{self.conversation_id}"
        
        self.collection.add(
            embeddings=[assistant_embedding],
            documents=[assistant_response],
            metadatas=[{
                "type": "assistant_response",
                "timestamp": timestamp,
                "conversation_id": self.conversation_id
            }],
            ids=[assistant_id]
        )
        
        self.conversation_id += 1
    
    def search_memory(self, query: str, n_results: int = 5) -> List[Dict]:
        """ê´€ë ¨ ê¸°ì–µ ê²€ìƒ‰"""
        query_embedding = self.create_embedding(query)
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=['documents', 'metadatas', 'distances']
        )
        
        memories = []
        for i, doc in enumerate(results['documents'][0]):
            memories.append({
                'content': doc,
                'metadata': results['metadatas'][0][i],
                'similarity': 1 - results['distances'][0][i]  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            })
        
        return memories
    
    def generate_response(self, user_input: str) -> str:
        """ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        # ê´€ë ¨ ê¸°ì–µ ê²€ìƒ‰
        memories = self.search_memory(user_input, n_results=3)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "ê´€ë ¨ ê³¼ê±° ëŒ€í™”:\n"
        for i, memory in enumerate(memories):
            if memory['similarity'] > 0.3:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                context += f"{i+1}. [{memory['metadata']['type']}] {memory['content']}\n"
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¹ì‹ ì€ ê³¼ê±° ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

{context}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
ë§Œì•½ ê´€ë ¨ëœ ê³¼ê±° ëŒ€í™”ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì–¸ê¸‰í•˜ë©° ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = self.llm.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def chat(self, user_input: str) -> str:
        """ëŒ€í™” ì²˜ë¦¬ (ê¸°ì–µ ê²€ìƒ‰ + ì‘ë‹µ ìƒì„± + ì €ì¥)"""
        # ì‘ë‹µ ìƒì„±
        response = self.generate_response(user_input)
        
        # ëŒ€í™” ì €ì¥
        self.store_conversation(user_input, response)
        
        return response
    
    def get_conversation_stats(self) -> Dict:
        """ëŒ€í™” í†µê³„ ì¡°íšŒ"""
        total_conversations = self.collection.count()
        return {
            "total_stored_messages": total_conversations,
            "conversation_pairs": total_conversations // 2
        }

# Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
def main():
    st.set_page_config(page_title="ëŒ€í™” ê¸°ì–µ ì—ì´ì „íŠ¸", page_icon="ğŸ¤–")
    
    st.title("ğŸ¤– ëŒ€í™” ê¸°ì–µ ì—ì´ì „íŠ¸")
    st.markdown("ê³¼ê±° ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ë§¥ë½ì„ ì´í•´í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸")
    
    # API í‚¤ ì…ë ¥
    if 'gemini_api_key' not in st.session_state:
        st.session_state.gemini_api_key = ""
    
    with st.sidebar:
        st.header("ì„¤ì •")
        api_key = st.text_input(
            "Google AI Studio API í‚¤", 
            value=st.session_state.gemini_api_key,
            type="password",
            help="https://aistudio.google.com/app/apikey ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”"
        )
        
        if api_key:
            st.session_state.gemini_api_key = api_key
        
        st.markdown("---")
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        if st.session_state.gemini_api_key:
            if 'agent' not in st.session_state:
                st.session_state.agent = MemoryAgent(st.session_state.gemini_api_key)
            
            # í†µê³„ í‘œì‹œ
            if 'agent' in st.session_state:
                stats = st.session_state.agent.get_conversation_stats()
                st.metric("ì €ì¥ëœ ëŒ€í™” ìŒ", stats['conversation_pairs'])
                st.metric("ì´ ë©”ì‹œì§€ ìˆ˜", stats['total_stored_messages'])
    
    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    if not st.session_state.gemini_api_key:
        st.warning("Google AI Studio API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.info("ğŸ”— API í‚¤ ë°œê¸‰: https://aistudio.google.com/app/apikey")
        return
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ìƒê°í•˜ëŠ” ì¤‘..."):
                response = st.session_state.agent.chat(prompt)
                st.markdown(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})
    
    # ê¸°ì–µ ê²€ìƒ‰ ê¸°ëŠ¥ (ë””ë²„ê¹…ìš©)
    with st.expander("ğŸ” ê¸°ì–µ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"):
        search_query = st.text_input("ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")
        if search_query and 'agent' in st.session_state:
            memories = st.session_state.agent.search_memory(search_query)
            for i, memory in enumerate(memories):
                st.write(f"**{i+1}. ìœ ì‚¬ë„: {memory['similarity']:.3f}**")
                st.write(f"íƒ€ì…: {memory['metadata']['type']}")
                st.write(f"ë‚´ìš©: {memory['content']}")
                st.write("---")

if __name__ == "__main__":
    main()