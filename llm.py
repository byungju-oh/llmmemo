# ëŒ€í™” ê¸°ì–µ ì—ì´ì „íŠ¸ êµ¬í˜„
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
# pip install --upgrade chromadb sentence-transformers google-generativeai streamlit

import os
import json
import datetime
from typing import List, Dict, Any, Generator
import chromadb
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import streamlit as st

class MemoryAgent:
    def __init__(self, gemini_api_key: str):
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¡œì»¬ ì €ì¥)
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        
        # ì»¬ë ‰ì…˜ ìƒì„±/ë¡œë“œ
        self.collection = self.chroma_client.get_or_create_collection(
            name="conversation_memory",
            metadata={"hnsw:space": "cosine"}
        )
        
        # ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Gemini API ì„¤ì •
        genai.configure(api_key=gemini_api_key)
        
        # [ìˆ˜ì • 1] ëª¨ë¸ ì´ë¦„ì„ ë” ëª…ì‹œì ì¸ 'gemini-1.0-pro'ë¡œ ë³€ê²½
        self.llm = genai.GenerativeModel('gemini-1.0-pro')
        
        # [ìˆ˜ì • 2] ì•ˆì „ ì„¤ì • ì •ì˜ (ê³¼ë„í•œ ì°¨ë‹¨ì„ ë§‰ê¸° ìœ„í•´ ë³´í†µ ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ì •)
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
        
        # ëŒ€í™” ID ì¹´ìš´í„°
        self.conversation_id = self.collection.count() // 2
    
    def create_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        return self.embedding_model.encode(text).tolist()
    
    def store_conversation(self, user_input: str, assistant_response: str):
        """ëŒ€í™”ë¥¼ ë²¡í„° DBì— ì €ì¥"""
        timestamp = datetime.datetime.now().isoformat()
        
        user_id = f"user_{self.conversation_id}"
        assistant_id = f"assistant_{self.conversation_id}"

        # ì‚¬ìš©ì ì…ë ¥ê³¼ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì„ í•œ ë²ˆì— ì¶”ê°€í•˜ì—¬ íš¨ìœ¨ì„± ì¦ëŒ€
        self.collection.add(
            embeddings=[
                self.create_embedding(user_input),
                self.create_embedding(assistant_response)
            ],
            documents=[user_input, assistant_response],
            metadatas=[
                {"type": "user_input", "timestamp": timestamp, "conversation_id": self.conversation_id},
                {"type": "assistant_response", "timestamp": timestamp, "conversation_id": self.conversation_id}
            ],
            ids=[user_id, assistant_id]
        )
        
        self.conversation_id += 1
    
    def search_memory(self, query: str, n_results: int = 5) -> List[Dict]:
        """ê´€ë ¨ ê¸°ì–µ ê²€ìƒ‰"""
        query_embedding = self.create_embedding(query)
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=['documents', 'metadatas', 'distances']
        )
        
        memories = []
        if not results['documents'][0]:
            return []

        for i, doc in enumerate(results['documents'][0]):
            memories.append({
                'content': doc,
                'metadata': results['metadatas'][0][i],
                'similarity': 1 - results['distances'][0][i]
            })
        
        return memories
    
    # [ìˆ˜ì • 3] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•´ ë°˜í™˜ íƒ€ì…ì„ Generatorë¡œ ë³€ê²½
    def generate_response_stream(self, user_input: str) -> Generator[str, None, None]:
        """ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„±"""
        memories = self.search_memory(user_input, n_results=3)
        
        context = "ê´€ë ¨ ê³¼ê±° ëŒ€í™”:\n"
        if memories:
            for i, memory in enumerate(memories):
                if memory['similarity'] > 0.3:
                    context += f"{i+1}. [{memory['metadata']['type']}] {memory['content']}\n"
        else:
            context += "ê´€ë ¨ëœ ê³¼ê±° ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

        prompt = f"""
ë‹¹ì‹ ì€ ê³¼ê±° ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

{context}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
ë§Œì•½ ê´€ë ¨ëœ ê³¼ê±° ëŒ€í™”ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì–¸ê¸‰í•˜ë©° ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        try:
            # stream=True ì˜µì…˜ ì‚¬ìš©
            response_stream = self.llm.generate_content(
                prompt,
                stream=True,
                safety_settings=self.safety_settings
            )
            for chunk in response_stream:
                # ì¼ë¶€ chunkì— textê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            yield f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # [ìˆ˜ì • 4] ìŠ¤íŠ¸ë¦¬ë°ì„ ì²˜ë¦¬í•˜ê³  ì „ì²´ ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” chat ë©”ì†Œë“œ
    def chat(self, user_input: str) -> Generator[str, None, None]:
        """ëŒ€í™” ì²˜ë¦¬ (ê¸°ì–µ ê²€ìƒ‰ + ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± + ì €ì¥)"""
        full_response_parts = []
        
        # ìŠ¤íŠ¸ë¦¼ì—ì„œ ì‘ë‹µì„ ë°›ì•„ì˜¤ë©´ì„œ yieldë¡œ ì „ë‹¬
        for chunk in self.generate_response_stream(user_input):
            full_response_parts.append(chunk)
            yield chunk

        # ì „ì²´ ì‘ë‹µì´ ì™„ë£Œë˜ë©´ ëŒ€í™” ì €ì¥
        full_response = "".join(full_response_parts)
        if not full_response.startswith("ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"):
             self.store_conversation(user_input, full_response)
    
    def get_conversation_stats(self) -> Dict:
        """ëŒ€í™” í†µê³„ ì¡°íšŒ"""
        total_messages = self.collection.count()
        return {
            "total_stored_messages": total_messages,
            "conversation_pairs": total_messages // 2
        }

# Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
def main():
    st.set_page_config(page_title="ëŒ€í™” ê¸°ì–µ ì—ì´ì „íŠ¸", page_icon="ğŸ¤–")
    
    st.title("ğŸ¤– ëŒ€í™” ê¸°ì–µ ì—ì´ì „íŠ¸")
    st.markdown("ê³¼ê±° ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ë§¥ë½ì„ ì´í•´í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸")
    
    if 'gemini_api_key' not in st.session_state:
        st.session_state.gemini_api_key = ""
    
    with st.sidebar:
        st.header("ì„¤ì •")
        api_key = st.text_input("Google AI Studio API í‚¤", value=st.session_state.gemini_api_key, type="password", help="https://aistudio.google.com/app/apikey ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”")
        
        if api_key:
            st.session_state.gemini_api_key = api_key
        
        st.markdown("---")
        
        if st.session_state.gemini_api_key:
            if 'agent' not in st.session_state:
                try:
                    st.session_state.agent = MemoryAgent(st.session_state.gemini_api_key)
                    st.success("ì—ì´ì „íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

            if 'agent' in st.session_state:
                stats = st.session_state.agent.get_conversation_stats()
                st.metric("ì €ì¥ëœ ëŒ€í™” ìŒ", stats['conversation_pairs'])
                st.metric("ì´ ë©”ì‹œì§€ ìˆ˜", stats['total_stored_messages'])
    
    if not st.session_state.gemini_api_key:
        st.warning("Google AI Studio API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.info("ğŸ”— API í‚¤ ë°œê¸‰: https://aistudio.google.com/app/apikey")
        return
    
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("assistant"):
            # [ìˆ˜ì • 5] st.write_streamì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í‘œì‹œ
            response_generator = st.session_state.agent.chat(prompt)
            full_response = st.write_stream(response_generator)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        # ì‚¬ì´ë“œë°” í†µê³„ ì¦‰ì‹œ ìƒˆë¡œê³ ì¹¨
        st.rerun()

    with st.expander("ğŸ” ê¸°ì–µ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"):
        search_query = st.text_input("ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")
        if search_query and 'agent' in st.session_state:
            with st.spinner("ê¸°ì–µ ê²€ìƒ‰ ì¤‘..."):
                memories = st.session_state.agent.search_memory(search_query)
                if not memories:
                    st.info("ê´€ë ¨ëœ ê¸°ì–µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                for i, memory in enumerate(memories):
                    st.write(f"**{i+1}. ìœ ì‚¬ë„: {memory['similarity']:.3f}**")
                    st.write(f"íƒ€ì…: {memory['metadata']['type']}")
                    st.write(f"ë‚´ìš©: {memory['content']}")
                    st.write("---")

if __name__ == "__main__":
    main()